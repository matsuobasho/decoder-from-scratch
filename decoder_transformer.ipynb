{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1710324713120
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1710324713280
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device set to: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device set to: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1710324713407
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, context_size, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoding = torch.zeros(context_size, d_model)\n",
        "\n",
        "        pos = torch.arange(0, context_size).unsqueeze(dim=1)\n",
        "        dim = torch.arange(\n",
        "            0, d_model, 2)  # dim is i in the positional encoding formula\n",
        "        self.encoding[:, 0::2] = torch.sin(pos / (10000**(2 * dim / d_model)))\n",
        "        self.encoding[:, 1::2] = torch.cos(pos / (10000**(2 * dim / d_model)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return self.encoding[:seq_len, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1710324713539
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1710324713714
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout_rate = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden_states, _ = self.self_attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(hidden_states)).to(device)\n",
        "        ff_output = self.feed_forward(x).to(device)\n",
        "        x = self.norm2(x + self.dropout(ff_output)).to(device)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1710324713844
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, context_size,\n",
        "                 d_model, d_ff, num_heads, n_blocks):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_size, d_model)\n",
        "        self.pos_embedding = PositionalEncoding(context_size, d_model)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DecoderBlock(\n",
        "                d_model=d_model,\n",
        "                num_heads=num_heads,\n",
        "                d_ff=d_ff,\n",
        "            )\n",
        "            for _ in range(n_blocks)\n",
        "        ])\n",
        "\n",
        "        self.out = nn.Linear(d_model, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).to(device) + self.pos_embedding(x).to(device)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        output = self.out(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1710324713981
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_size,\n",
        "                 d_model, d_ff, num_heads, n_blocks):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size,\n",
        "            context_size,\n",
        "            d_model,\n",
        "            d_ff,\n",
        "            num_heads,\n",
        "            n_blocks\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.decoder(x)  # input_decoder shape - (64, 99)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1710324714111
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "## Reading and processing text\n",
        "\n",
        "# Download 'The Mysterious Island' from\n",
        "# https://www.gutenberg.org/cache/epub/1268/pg1268.txt\n",
        "with open('data/1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
        "    text=fp.read()\n",
        "\n",
        "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "\n",
        "text = text[start_indx:end_indx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1710324716428
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "text_encoded = enc.encode(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Train model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1710324716989
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "CONTEXT_SIZE = 40\n",
        "chunk_size = CONTEXT_SIZE  + 1\n",
        "\n",
        "# n chunks where each next chunk is 1 word offset from the previous chunk\n",
        "token_chunks = [\n",
        "    text_encoded[i:i + chunk_size]\n",
        "    for i in range(len(text_encoded) - chunk_size + 1)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1710324724986
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(token_chunks).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1710324725124
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 50\n",
        "seq_dl = DataLoader(seq_dataset,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    shuffle=True,\n",
        "                    drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1710324725251
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 104000\n",
        "D_MODEL = 500\n",
        "D_FF = 20\n",
        "NUM_HEADS = 10\n",
        "N_BLOCKS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1710324726077
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(104000, 500)\n",
              "    (pos_embedding): PositionalEncoding()\n",
              "    (blocks): ModuleList(\n",
              "      (0-9): 10 x DecoderBlock(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=500, out_features=500, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=500, out_features=20, bias=True)\n",
              "          (linear2): Linear(in_features=20, out_features=500, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (norm1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((500,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (out): Linear(in_features=500, out_features=104000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    context_size=CONTEXT_SIZE,\n",
        "    d_model=D_MODEL,\n",
        "    d_ff=D_FF,  # internal dimension of the feed forward network\n",
        "    num_heads=NUM_HEADS,\n",
        "    n_blocks=N_BLOCKS)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1710324726307
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Roma\\.virtualenvs\\decoder-from-scratch-1iDsID9S\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(104000, 500)\n",
              "    (pos_embedding): PositionalEncoding()\n",
              "    (blocks): ModuleList(\n",
              "      (0-9): 10 x DecoderBlock(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=500, out_features=500, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=500, out_features=20, bias=True)\n",
              "          (linear2): Linear(in_features=20, out_features=500, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (norm1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((500,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (out): Linear(in_features=500, out_features=104000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1710324726443
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1710324726571
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# loss_avg = [0] * NUM_EPOCHS\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     for src_data, tgt_data in seq_dl:\n",
        "#         output = model(src_data)\n",
        "#         loss = criterion(output.view(-1, VOCAB_SIZE), tgt_data.view(-1))\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n",
        "#         loss_avg[epoch] += loss.item()\n",
        "#     loss_avg[epoch] /= BATCH_SIZE\n",
        "#     print(f\"Epoch: {epoch+1}, Loss: {loss_avg[epoch]}\")\n",
        "\n",
        "#     torch.save(model.state_dict(), f\"model_epoch_{epoch+7}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Predict on New Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load model weights\n",
        "\n",
        "# trained on GPU but performing inference locally on CPU\n",
        "MODEL_PATH = \"data/model_epoch_10.pth\"\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "gather": {
          "logged": 1710325114374
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# prediction_start = \"And then they decided to embark on a new adventure.  They began to make preparations\"\n",
        "\n",
        "# prediction_start = \"Sheila noticed her emotions\"\n",
        "\n",
        "#prediction_start = \"The sea was calm that \"\n",
        "\n",
        "prediction_start = \"Harding wondered \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "gather": {
          "logged": 1710324734414
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(104000, 500)\n",
              "    (pos_embedding): PositionalEncoding()\n",
              "    (blocks): ModuleList(\n",
              "      (0-9): 10 x DecoderBlock(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=500, out_features=500, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=500, out_features=20, bias=True)\n",
              "          (linear2): Linear(in_features=20, out_features=500, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (norm1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((500,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (out): Linear(in_features=500, out_features=104000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Greedy search prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "gather": {
          "logged": 1710325185296
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "tokenized_input = torch.tensor(enc.encode(prediction_start)).to(device)\n",
        "tokenized_input = tokenized_input[None,:] # add dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Greedy search\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(15):\n",
        "    res = model(tokenized_input)\n",
        "    next_token = torch.argmax(res[:,-1,:], dim=1)\n",
        "    tokenized_input = torch.cat([tokenized_input, next_token[None,:]], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Harding wondered  that  that  was  was  was  wouldChapter Chapter '"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc.decode(tokenized_input.squeeze().tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multinomial prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_input = torch.tensor(enc.encode(prediction_start)).to(device)\n",
        "tokenized_input = tokenized_input[None,:] # add dimension\n",
        "\n",
        "m = torch.nn.Softmax(dim=2)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(10):\n",
        "    res = model(tokenized_input)\n",
        "    res_probs = m(res)\n",
        "    next_token = torch.multinomial(res_probs[:,-1,:], 1)\n",
        "    tokenized_input = torch.cat([tokenized_input, next_token], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "gather": {
          "logged": 1710325186115
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Harding wondered mouth resigned“And Your litter intercepted condemn rigging discovery'"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc.decode(tokenized_input.squeeze().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "display_name": "decoder-from-scratch-1iDsID9S",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
